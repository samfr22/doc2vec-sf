{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Custom Doc2Vec for Movie Plot Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset and clean the text from any punctuation, numbers, and make lowercase.\n",
    "As this happens, build the test and train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "import string\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "\n",
    "# Taken from ntlk.corpus.stopwords(\"english\") without installing the package\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean a document's text by removing punctuation, numbers, and making lower\n",
    "    case\n",
    "    \n",
    "    :param text: The contents of one document\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    cleaned = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        temp_word = []\n",
    "        for letter in word:\n",
    "            if letter not in string.punctuation and letter not in string.digits:\n",
    "                temp_word.append(letter)\n",
    "        temp_word = \"\".join(temp_word)\n",
    "        if len(temp_word) > 0 and not temp_word in stop_words:\n",
    "            cleaned.append(temp_word)\n",
    "        \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc:\n",
    "    \"\"\"\n",
    "    Class represents a singular document and contains the relevant information\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: list, name:str, label: str):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        self.name = name\n",
    "        self.index = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        text = \" \".join(self.text)\n",
    "        return \"[\" + text + \"],\" + \"[\" + label + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>[little, boy, named, andy, loves, room, playin...</td>\n",
       "      <td>animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>[two, kids, find, play, magical, board, game, ...</td>\n",
       "      <td>fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>[things, dont, seem, change, much, wabasha, co...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heat (1995)</td>\n",
       "      <td>[hunters, preyneil, professional, criminal, cr...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sabrina (1995)</td>\n",
       "      <td>[ugly, duckling, undergone, remarkable, change...</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Harold and Kumar Go to White Castle (2004)</td>\n",
       "      <td>[asianamerican, office, worker, indian, stoner...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Princess Diaries 2: Royal Engagement, The (2004)</td>\n",
       "      <td>[princess, mia, turned, supposed, succeed, gra...</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>AVP: Alien vs. Predator (2004)</td>\n",
       "      <td>[archaeological, expedition, bouvetya, island,...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Yu-Gi-Oh! (2004)</td>\n",
       "      <td>[underneath, sands, egypt, anubis, ancient, ev...</td>\n",
       "      <td>animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Cellular (2004)</td>\n",
       "      <td>[young, man, receives, emergency, phone, call,...</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                                    Toy Story (1995)   \n",
       "1                                      Jumanji (1995)   \n",
       "2                             Grumpier Old Men (1995)   \n",
       "3                                         Heat (1995)   \n",
       "4                                      Sabrina (1995)   \n",
       "..                                                ...   \n",
       "995        Harold and Kumar Go to White Castle (2004)   \n",
       "996  Princess Diaries 2: Royal Engagement, The (2004)   \n",
       "997                    AVP: Alien vs. Predator (2004)   \n",
       "998                                  Yu-Gi-Oh! (2004)   \n",
       "999                                   Cellular (2004)   \n",
       "\n",
       "                                                  plot      genre  \n",
       "0    [little, boy, named, andy, loves, room, playin...  animation  \n",
       "1    [two, kids, find, play, magical, board, game, ...    fantasy  \n",
       "2    [things, dont, seem, change, much, wabasha, co...     comedy  \n",
       "3    [hunters, preyneil, professional, criminal, cr...     action  \n",
       "4    [ugly, duckling, undergone, remarkable, change...    romance  \n",
       "..                                                 ...        ...  \n",
       "995  [asianamerican, office, worker, indian, stoner...     comedy  \n",
       "996  [princess, mia, turned, supposed, succeed, gra...    romance  \n",
       "997  [archaeological, expedition, bouvetya, island,...     action  \n",
       "998  [underneath, sands, egypt, anubis, ancient, ev...  animation  \n",
       "999  [young, man, receives, emergency, phone, call,...     action  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# possible_tags = [\"sci-fi\", \"animation\", \"action\", \"comedy\", \"fantasy\", \"romance\"]\n",
    "\n",
    "# Keep track of the original format of docs for visual output later?\n",
    "corpus = []\n",
    "docs = []\n",
    "data = {\"title\": [], \"plot\": [], \"genre\": []}\n",
    "\n",
    "# Restrict the number of docs, since my computer crashes with too many\n",
    "allowed = 1000\n",
    "with open(\"movie_plots.csv\") as file:\n",
    "    reader = csv.reader(file, delimiter = \",\", quotechar = '\"')\n",
    "    for row in reader:\n",
    "        if allowed <= 0:\n",
    "            break\n",
    "        cleaned = clean_text(row[2])\n",
    "        data[\"title\"].append(row[1])\n",
    "        data[\"plot\"].append(cleaned)\n",
    "        data[\"genre\"].append(row[3])\n",
    "        docs.append(Doc(text = cleaned, name = row[1], label = row[3]))\n",
    "        allowed -= 1\n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words: 10714\n",
      "Num docs: 1000\n"
     ]
    }
   ],
   "source": [
    "# Get some of the features from the dataset\n",
    "unique_words = []\n",
    "for doc in docs:\n",
    "    for word in doc.text:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "vocab_size = len(unique_words)\n",
    "num_docs = len(docs)\n",
    "\n",
    "print(\"Num words:\", vocab_size)\n",
    "print(\"Num docs:\", num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make vector encodings for all words and docs\n",
    "words_copy = unique_words.copy()\n",
    "docs_copy = docs.copy()\n",
    "word_enc_mapping = {}\n",
    "doc_enc_mapping = {}\n",
    "\n",
    "for i in range(num_docs):\n",
    "    doc_enc_mapping[docs[i].name] = i\n",
    "    docs_copy[i] = doc_enc_mapping[docs[i].name]\n",
    "    \n",
    "for i in range(vocab_size):\n",
    "    word_enc_mapping[unique_words[i]] = i\n",
    "    words_copy[i] = word_enc_mapping[unique_words[i]]\n",
    "\n",
    "doc_vecs = tf.keras.utils.to_categorical(docs_copy)\n",
    "word_vecs = tf.keras.utils.to_categorical(words_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for model building\n",
    "batch_size = 64\n",
    "embed_size = 128\n",
    "negative_samples = 8\n",
    "learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "text_input = keras.Input(shape = [batch_size, 1], dtype = tf.int32)\n",
    "# label_input = keras.Input(shape = [batch_size, 1], dtype = tf.int32)\n",
    "doc_vector = keras.Input(shape = [batch_size, 1], dtype = tf.int32)\n",
    "\n",
    "# Embeddings\n",
    "init_embed = keras.initializers.RandomUniform(minval=-1.0, maxval=1.0)\n",
    "\n",
    "embed_for_word = keras.layers.Embedding(vocab_size, embed_size, embeddings_initializer = init_embed, input_length = batch_size)\n",
    "word_embed = embed_for_word(text_input)\n",
    "\n",
    "embed_for_docs = keras.layers.Embedding(num_docs, embed_size, embeddings_initializer = init_embed, input_length = batch_size)\n",
    "doc_embed = embed_for_docs(doc_vector)\n",
    "\n",
    "# Combine embeddings and flatten\n",
    "combined = keras.layers.Concatenate(axis = 1)([word_embed, doc_embed])\n",
    "flattened = keras.layers.Flatten()(combined)\n",
    "\n",
    "# Softmax activation\n",
    "softmax = keras.layers.Dense(vocab_size, activation = \"softmax\")\n",
    "activated = softmax(flattened)\n",
    "\n",
    "output = keras.layers.Dense(1, activation = \"sigmoid\")(activated)\n",
    "\n",
    "model = keras.Model(inputs = [text_input, doc_vector], outputs = output)\n",
    "\n",
    "# SGD Optimizer and CCE Loss\n",
    "sgd = keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = sgd, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "Create functions that generate batch data since dimensions don't match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Time to see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [142]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     batch_frame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([batch_frame, temp_frame], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m batch_frame \u001b[38;5;241m=\u001b[39m batch_frame\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_frame\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs3907_NN/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/cs3907_NN/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.float32)."
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "doc_index = 0\n",
    "for epoch in range(epochs):\n",
    "    # Build a batch for the next epoch\n",
    "    # x is combined doc_id and word_ids\n",
    "    batch = []\n",
    "    batch_frame = pd.DataFrame()\n",
    "    for i in range(batch_size):\n",
    "        id_batch = []\n",
    "        doc_batch = []\n",
    "        label_batch = []\n",
    "        for j in range(window_size):\n",
    "            docs[doc_index].index = (docs[doc_index].index + 1) % len(docs[doc_index].text)\n",
    "            doc_batch.append(word_encs[0][word_enc_mapping[docs[doc_index].text[docs[doc_index].index]]])\n",
    "            id_batch.append(doc_encs[0][doc_enc_mapping[docs[doc_index].name]])\n",
    "            label_batch.append(docs[doc_index].label)\n",
    "        doc_index = (doc_index + 1) % num_docs\n",
    "        \n",
    "        id_batch = np.array(id_batch)\n",
    "        id_batch_tf = tf.convert_to_tensor(id_batch)\n",
    "        print(id_batch_tf)\n",
    "        doc_batch = np.array(doc_batch)\n",
    "        label_batch = np.array(label_batch)\n",
    "        \n",
    "        temp_frame = pd.DataFrame([id_batch, doc_batch, label_batch])\n",
    "        batch_frame = pd.concat([batch_frame, temp_frame], axis = 1)\n",
    "    batch_frame = batch_frame.transpose()\n",
    "    x, y = np.split(batch_frame, [2], axis = 1)\n",
    "    model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs3907_NN] *",
   "language": "python",
   "name": "conda-env-cs3907_NN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f7289dec51d54971fbf4142208acf0e9b624e3773a81ef6b1635e6f8d990aec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
